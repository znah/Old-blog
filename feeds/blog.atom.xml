<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Alexander Mordvintsev</title><link href="http://znah.net/" rel="alternate"></link><link href="http://znah.net/feeds/blog.atom.xml" rel="self"></link><id>http://znah.net/</id><updated>2013-01-04T00:00:00+01:00</updated><entry><title>Kinect + PS3 Move</title><link href="http://znah.net/kinect-ps3-move.html" rel="alternate"></link><updated>2013-01-04T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2013-01-04:kinect-ps3-move.html</id><summary type="html">
&lt;div class="separator" style="clear: both; text-align: left;"&gt;Once I happened to have both MS &lt;a href="http://en.wikipedia.org/wiki/Kinect"&gt;Kinect&lt;/a&gt; and &lt;a href="http://en.wikipedia.org/wiki/PlayStation_Move"&gt;PS3 Move&lt;/a&gt;&amp;nbsp;on my table. I connected both of them to my PC (trivial for Kinect and ugly for Move). Then I tried to calibrate both cameras to match their coordinate frames. Here is the result I managed to get:&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: left;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://0.gvt0.com/vi/O4VqGUbNG7g/0.jpg" height="266" width="320"&gt;&lt;param name="movie" value="http://www.youtube.com/v/O4VqGUbNG7g&amp;amp;fs=1&amp;amp;source=uds" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/O4VqGUbNG7g&amp;amp;fs=1&amp;amp;source=uds" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;br /&gt;There is some noticeable latency between Move and Kinect data streams. Also the setup for connecting PS3 Move to PC was so ugly and&amp;nbsp;inconvenient&amp;nbsp;(required a full PlayStation 3 and $100 Move.Me app for it) that I abandoned that experiment. Shame on you, Sony.&lt;br /&gt;&lt;br /&gt;Recently I stumbled upon the &lt;a href="http://code.google.com/p/moveonpc/"&gt;MoveOnPC&lt;/a&gt; project, which can probably be used to make a setup like mine, but more usable.</summary></entry><entry><title>Coral growth (and voxels)</title><link href="http://znah.net/coral-growth-and-voxels.html" rel="alternate"></link><updated>2012-11-07T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2012-11-07:coral-growth-and-voxels.html</id><summary type="html">
Before I learned about computer vision my main&amp;nbsp;interests&amp;nbsp;where computer graphics and&amp;nbsp;computational&amp;nbsp;modeling of&amp;nbsp;physical and biological systems. At the time I was&amp;nbsp;playing with voxel graphics I wanted to find a source of interesting, complex and natural-looking geometry. My&amp;nbsp;adviser pointed me to the work&amp;nbsp;Roeland Merks on&amp;nbsp;&lt;a href="http://homepages.cwi.nl/~merks/Publications/Corals.html"&gt;coral growth simulation&lt;/a&gt;. I implemented a version of his model and recorded couple of growth videos:&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://2.gvt0.com/vi/KMeLerOrbHo/0.jpg" height="266" width="320"&gt;&lt;param name="movie" value="http://www.youtube.com/v/KMeLerOrbHo&amp;amp;fs=1&amp;amp;source=uds" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/KMeLerOrbHo&amp;amp;fs=1&amp;amp;source=uds" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://2.gvt0.com/vi/QsUm42DJQZI/0.jpg" height="266" width="320"&gt;&lt;param name="movie" value="http://www.youtube.com/v/QsUm42DJQZI&amp;amp;fs=1&amp;amp;source=uds" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/QsUm42DJQZI&amp;amp;fs=1&amp;amp;source=uds" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: left;"&gt;Later I put the polygonal mesh produced by simulation into my voxel graphics engine (hope to write about it later):&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: left;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: center;"&gt;&lt;object width="320" height="266" class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://2.gvt0.com/vi/qNRkXfEWum4/0.jpg"&gt;&lt;param name="movie" value="http://www.youtube.com/v/qNRkXfEWum4&amp;amp;fs=1&amp;amp;source=uds" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/qNRkXfEWum4&amp;amp;fs=1&amp;amp;source=uds" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;div style="text-align: left;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style="text-align: left;"&gt;May be one day I'll make sources of my coral growth model less messy and more&amp;nbsp;convenient&amp;nbsp;to use release them.&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;
</summary></entry><entry><title>deconvolution.py</title><link href="http://znah.net/deconvolutionpy.html" rel="alternate"></link><updated>2012-11-06T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2012-11-06:deconvolutionpy.html</id><summary type="html">
  &lt;div class="separator" style="clear: both; text-align: left;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both;"&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;&lt;a href="https://github.com/Itseez/opencv/blob/master/samples/python2/deconvolution.py"&gt;deconvolution.py&lt;/a&gt;&lt;/span&gt;&amp;nbsp;sample shows how DFT can be used to perform &lt;a href="http://en.wikipedia.org/wiki/Wiener_deconvolution"&gt;Weiner deconvolution&lt;/a&gt;&amp;nbsp;of an image with user-defined point spread function (PSF). First I tested my implementation on an &lt;a href="http://www.topazlabs.com/infocus/_images/licenseplate_compare.jpg"&gt;image&lt;/a&gt;&amp;nbsp;from &lt;a href="http://www.topazlabs.com/infocus/#3"&gt;Topaz InFocus examples&lt;/a&gt;. The result (on the image bellow) was quite impressive, but the low level of noise and the simplicity of PSF suggest that the initial image might have been distorted&amp;nbsp;manually.&lt;/div&gt;&lt;div class="separator" style="clear: both;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-ZrBnB8RFpEw/UJePA6LjnDI/AAAAAAAAAmc/kMgwCURQhs8/s1600/deconv_car.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="256" src="http://3.bp.blogspot.com/-ZrBnB8RFpEw/UJePA6LjnDI/AAAAAAAAAmc/kMgwCURQhs8/s640/deconv_car.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Then I decided to try my program on a couple of crops from shots I made with a compact digital camera. I intentionally shook the camera and applied wrong focus settings to get blurry images. See results below. My aim was to show that deconvolution can sometimes transform unreadable text into readable.&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-gG8s6BIibiE/UJePB5j6SUI/AAAAAAAAAmg/FsWm14b65YU/s1600/deconv_text_blur.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="216" src="http://3.bp.blogspot.com/-gG8s6BIibiE/UJePB5j6SUI/AAAAAAAAAmg/FsWm14b65YU/s640/deconv_text_blur.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://2.bp.blogspot.com/-mitC-uOMFz4/UJePCkNPneI/AAAAAAAAAmo/ugqId3jX7Do/s1600/deconv_text_motion.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="196" src="http://2.bp.blogspot.com/-mitC-uOMFz4/UJePCkNPneI/AAAAAAAAAmo/ugqId3jX7Do/s640/deconv_text_motion.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Note that the user is expected to manually tune the PSF for proper image restoration. Parameter presets for images shown here can be found in the sample's doc string.
</summary></entry><entry><title>Plane tracking with Python OpenCV</title><link href="http://znah.net/plane-tracking-with-python-opencv.html" rel="alternate"></link><updated>2012-11-05T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2012-11-05:plane-tracking-with-python-opencv.html</id><summary type="html">
  &lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;When I was working Python samples for OpenCV I decided to try to reproduce &lt;a href="http://www.youtube.com/watch?v=-ZNYoL8rzPY"&gt;this&lt;/a&gt;&amp;nbsp;real-time plane tracking demo using Features2D framework. Here is a couple of videos showing the results:&lt;br /&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://3.gvt0.com/vi/FirtmYcC0Vc/0.jpg" height="266" width="320"&gt;&lt;param name="movie" value="http://www.youtube.com/v/FirtmYcC0Vc&amp;amp;fs=1&amp;amp;source=uds" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/FirtmYcC0Vc&amp;amp;fs=1&amp;amp;source=uds" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;object class="BLOGGER-youtube-video" classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,40,0" data-thumbnail-src="http://i.ytimg.com/vi/pzVbhxx6aog/0.jpg" height="266" width="320"&gt;&lt;param name="movie" value="http://www.youtube.com/v/pzVbhxx6aog?version=3&amp;amp;f=user_uploads&amp;amp;c=google-webdrive-0&amp;amp;app=youtube_gdata" /&gt;&lt;param name="bgcolor" value="#FFFFFF" /&gt;&lt;param name="allowFullScreen" value="true" /&gt;&lt;embed width="320" height="266" src="http://www.youtube.com/v/pzVbhxx6aog?version=3&amp;amp;f=user_uploads&amp;amp;c=google-webdrive-0&amp;amp;app=youtube_gdata" type="application/x-shockwave-flash" allowfullscreen="true"&gt;&lt;/embed&gt;&lt;/object&gt;&lt;/div&gt;&lt;br /&gt;I use ORB feature&amp;nbsp;detector/descriptor&amp;nbsp;and FLANN-based descriptor matcher. Also &lt;span style="font-family: Courier New, Courier, monospace;"&gt;cv2.solvePnP&lt;/span&gt;&lt;span style="font-family: inherit;"&gt;&amp;nbsp;function is used to estimate camera position relative to the tracked plane for simple video augmentation. Relevant &lt;a href="https://github.com/Itseez/opencv/tree/master/samples/python2"&gt;codes&lt;/a&gt; are located in files:&lt;/span&gt;&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/Itseez/opencv/blob/master/samples/python2/plane_tracker.py"&gt;plane_tracker.py&lt;/a&gt; - tracker implementation and multiple targets demo&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/Itseez/opencv/blob/master/samples/python2/feature_homography.py"&gt;feature_homography.py&lt;/a&gt; - split screen example from the first video&lt;/li&gt;&lt;li&gt;&lt;a href="https://github.com/Itseez/opencv/blob/master/samples/python2/plane_ar.py"&gt;plane_ar.py&lt;/a&gt; - simple plane based augmented reality test&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;
</summary></entry><entry><title>Python samples for OpenCV: ASIFT (GSoC project)</title><link href="http://znah.net/python-samples-for-opencv-asift-gsoc-project.html" rel="alternate"></link><updated>2012-11-01T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2012-11-01:python-samples-for-opencv-asift-gsoc-project.html</id><summary type="html">
  &lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;I often use python for doing experiments in computer vision and other topics. It allows me to simply open a&lt;br /&gt;text editor and type:&lt;br /&gt;&lt;blockquote class="tr_bq"&gt;&lt;span style="font-family: Courier New, Courier, monospace;"&gt;import numpy as np&lt;br /&gt;import cv2&lt;br /&gt;&amp;lt;...and some fancy CV code goes here&amp;gt;&lt;/span&gt;&lt;/blockquote&gt;No messing with projects and build systems. If I need some filter design or non-linear optimization - &lt;a href="http://www.scipy.org/"&gt;SciPy&lt;/a&gt;.&amp;nbsp;For advanced plotting there is &lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;. And most of the stuff that is needed apart from pure python is bundled&amp;nbsp;into the &lt;a href="http://www.enthought.com/products/epd_free.php"&gt;EPD-free&lt;/a&gt; distribution.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;I participated in &lt;a href="http://code.google.com/intl/ru/soc/"&gt;Google Summer of Code&lt;/a&gt; program in years 2011 and 2012. My objective&amp;nbsp;was making &lt;a href="https://github.com/Itseez/opencv/tree/master/samples/python2"&gt;samples&lt;/a&gt; of using OpenCV Python API and improving the API itself. Besides making new code samples I:&lt;br /&gt;&lt;br /&gt;&lt;ul style="text-align: left;"&gt;&lt;li&gt;made some previously unexposed parts of OpenCV (like &lt;a href="http://docs.opencv.org/modules/features2d/doc/features2d.html"&gt;Features2D&lt;/a&gt; framework) available from Python bindings,&lt;/li&gt;&lt;li&gt;made OpenCV functions release Python's GIL, alowing for multithreaded image processing in python scripts,&lt;/li&gt;&lt;li&gt;fixed some critical bugs and more...&lt;/li&gt;&lt;/ul&gt;&lt;h3 style="text-align: left;"&gt;ASIFT&lt;/h3&gt;&lt;div&gt;&lt;a href="http://www.cmap.polytechnique.fr/~yu/research/ASIFT/demo.html"&gt;ASIFT&lt;/a&gt; is a method of improving feature-based image matching by sampling various affine transformations of matched images. This strategy can be implemented on top of any scale and rotation invariant local image features. The authors of the method adopted &lt;a href="http://en.wikipedia.org/wiki/Scale-invariant_feature_transform"&gt;SIFT&lt;/a&gt; for their reference implementation. Here is &lt;a href="https://github.com/Itseez/opencv/blob/master/samples/python2/asift.py"&gt;my ASIFT implementation&lt;/a&gt;, created as a part of GSoC.&amp;nbsp;Image below shows the result of its application to a pair of images taken from an airplane. It manages to find 42 matches on an image pair I find hard to match even for humans!&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;/div&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="http://3.bp.blogspot.com/-IBeL2jAZyzM/UI_6H2VrDvI/AAAAAAAAAmM/6O61ylqw4Hs/s1600/asift.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"&gt;&lt;img border="0" height="240" src="http://3.bp.blogspot.com/-IBeL2jAZyzM/UI_6H2VrDvI/AAAAAAAAAmM/6O61ylqw4Hs/s640/asift.png" width="640" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;
</summary></entry><entry><title>My first youtube video</title><link href="http://znah.net/my-first-youtube-video.html" rel="alternate"></link><updated>2012-10-30T00:00:00+01:00</updated><author><name>Alexander Mordvintsev</name></author><id>tag:znah.net,2012-10-30:my-first-youtube-video.html</id><summary type="html">&lt;p&gt;In a year 2007 I was doing a small course project on cellular
automaton simulation of physical phenomena. I was intrigued by the
simplicity of Lattice-Gas model and its ability to capture the complex
stuff that happens in fluid flows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"We have noticed in nature that the behavior of a fluid depends very
little on the nature of the individual particles in that fluid. For
example, the flow of sand is very similar to the flow of water or the
flow of a pile of ball bearings. We have therefore taken advantage of
this fact to invent a type of imaginary particle that is especially
simple for us to simulate. This particle is a perfect ball bearing that
can move at a single speed in one of six directions. The flow of these
particles on a large enough scale is very similar to the flow of natural
fluids."&lt;/p&gt;
&lt;p&gt;Richard Feynman
&lt;a href="&amp;quot;http://longnow.org/essays/richard-feynman-connection-machine/"&gt;(source)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It tried to implement an extension of Lattice-Gas called
Lattice-Boltzmann Model and recorded a small video of the Von Karman
vortex street formation:&lt;/p&gt;
&lt;p&gt;http://www.youtube.com/v/CB2aWiesq0g&lt;/p&gt;
&lt;p&gt;&lt;a href="http://en.wikipedia.org/wiki/File:Vortex-street-1.jpg"&gt;
&lt;img style="float:right; margin:10px" alt="Vortex Street in Clouds" src="http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Vortex-street-1.jpg/134px-Vortex-street-1.jpg"/&gt;
&lt;/a&gt;
I wanted to reproduce this vortex pattern in simulation since I saw this 
beautiful picture on Wikipedia. Later that year I implemented some
extensions to the model to make it stable on higher Reynolds number
flows and ported it on GPU. Seeing my simulation running more then 10x
faster on a rather modest laptop's video card and being able to play
with it in real-time turned me into a fan of GPU computing. Here is one
more video where I tuned the flow to make it more chaotic. Also I there
is a &lt;a href="http://is.ifmo.ru/download/lattice_boltzmann.pdf"&gt;report (in russian)&lt;/a&gt; about this work.&lt;/p&gt;
&lt;p&gt;http://www.youtube.com/v/8xwULctHPZs&lt;/p&gt;
&lt;p&gt;[vortex_cloud] http://upload.wikimedia.org/wikipedia/commons/thumb/2/2c/Vortex-street-1.jpg/334px-Vortex-street-1.jpg&lt;/p&gt;</summary></entry></feed>